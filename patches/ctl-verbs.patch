Bottom: e2d45d07a36bfb4ff0988a8fddff1fd84009996e
Top:    4d9b3321af75b907602542b4d4ceceb3094017dc
Author: Alexey Lyashkov <shadow@Alexeys-MacBook-Pro.local>
Date:   2016-01-21 10:12:50 +0300

attach control message to IB

use IB verbs api to attach control messages
to incomming ring.
Send ctl mgs on wire to verify recv code works



---

diff --git a/cli.c b/cli.c
index f0bcd39..aa5a5a0 100644
--- a/cli.c
+++ b/cli.c
@@ -36,8 +36,16 @@ cli_init(void)
 		printk("error connect \n");
 		goto exit;
 	}
-	event = ib_socket_poll(sock);
-	printk("Event hit %lx\n", event);
+	while ( 1 ) {
+		event = ib_socket_poll(sock);
+		printk("Event hit %lx\n", event);
+		if (event & POLLERR)
+			break;
+		if (event & POLLOUT) {
+			err = ib_socket_write(sock, srv_addr, strlen(srv_addr));
+			printk("sends status %d\n", err);
+		}
+	}
 
 	ib_socket_disconnect(sock);
 exit:
diff --git a/ib-sock-ctl.c b/ib-sock-ctl.c
index 4f35dcd..ed4582e 100644
--- a/ib-sock-ctl.c
+++ b/ib-sock-ctl.c
@@ -42,7 +42,26 @@ ib_sock_ctl_take(struct IB_SOCK *sock)
  */
 int ib_sock_ctl_post(struct IB_SOCK *sock, struct ib_sock_ctl *msg)
 {
-	return 0;
+	/* base on ib_mad_post_receive_mads() - that wr isn't need to live any time 
+	 * so - save memory and declare on stack.
+	 */
+	struct ib_recv_wr wr;
+	struct ib_recv_wr *bad_wr;
+	struct ib_device *device = sock->is_id->device;
+
+	/* trasnfer ownership to the device */
+	ib_dma_sync_single_for_device(device, 
+				   msg->iscm_sge.addr, msg->iscm_sge.length,
+				   DMA_FROM_DEVICE);
+
+	msg->iscm_flags |= CTL_MSG_RX;
+
+	wr.next = NULL;
+	wr.wr_id = (uintptr_t)msg;
+	wr.sg_list = &msg->iscm_sge;
+	wr.num_sge = 1;
+
+	return ib_post_recv(sock->is_qp, &wr, &bad_wr);
 }
  
 void ib_sock_ctl_put(struct IB_SOCK *sock, struct ib_sock_ctl *msg)
@@ -54,15 +73,45 @@ void ib_sock_ctl_put(struct IB_SOCK *sock, struct ib_sock_ctl *msg)
 	wake_up(&sock->is_ctl_waitq);
 }
 
+static int 
+ctl_msg_init(struct IB_SOCK *sock, struct ib_sock_ctl *msg)
+{
+	struct ib_device *device = sock->is_id->device;
+	unsigned long dma_addr;
+
+	dma_addr = ib_dma_map_single(device, (void *)&msg->iscm_msg,
+				sizeof(msg->iscm_msg), DMA_FROM_DEVICE);
+	if (ib_dma_mapping_error(device, dma_addr))
+		return -EIO;
+
+	msg->iscm_flags = 0;
+	msg->iscm_sge.addr = dma_addr;
+	msg->iscm_sge.length = sizeof(msg->iscm_msg);
+	msg->iscm_sge.lkey   = sock->is_mem.ism_mr->lkey;
+
+	return 0;
+}
+
+static void
+ctl_msg_fini(struct IB_SOCK *sock, struct ib_sock_ctl *msg)
+{
+	struct ib_device *device = sock->is_id->device;
+
+	ib_dma_unmap_single(device, msg->iscm_sge.addr,
+			    sizeof(msg->iscm_msg), DMA_FROM_DEVICE);
+}
+
 int ib_sock_ctl_init(struct IB_SOCK *sock)
 {
 	struct ib_sock_ctl *msg;
 	unsigned int i;
 	unsigned count = 0;
+	int ret;
 
 	init_waitqueue_head(&sock->is_ctl_waitq);
 	INIT_LIST_HEAD(&sock->is_ctl_active_list);
 	INIT_LIST_HEAD(&sock->is_ctl_idle_list);
+	INIT_LIST_HEAD(&sock->is_ctl_rd_list);
 
 	/* preallocate until limit.
 	 * we can allocate it by request, but simplify a code
@@ -71,15 +120,23 @@ int ib_sock_ctl_init(struct IB_SOCK *sock)
 		msg = kmalloc(sizeof(*msg), GFP_KERNEL);
 		if (msg == NULL)
 			continue;
-		count ++;
 		/* pre init */
+		if (ctl_msg_init(sock, msg) != 0) {
+			kfree(msg);
+			continue;
+		}
+		count ++;
 		ib_sock_ctl_put(sock, msg);
 	}
 	/* half of mgs uses for incomming, half outgoning */
 	count /= 2;
 	for(i = 0; i < count; i++) { 
 		msg = ib_sock_ctl_take(sock);
-		ib_sock_ctl_post(sock, msg);
+		ret = ib_sock_ctl_post(sock, msg);
+		if (ret != 0) {
+			msg->iscm_flags  = 0;
+			ib_sock_ctl_put(sock, msg);
+		}
 	}
 
 	return 0;
@@ -89,15 +146,17 @@ void ib_sock_ctl_fini(struct IB_SOCK *sock)
 {
 	struct ib_sock_ctl *pos, *next;
 
-	/* XXX flush active */
+	/* if QP destroyed it is safe to unmap memory and free */
 	list_for_each_entry_safe(pos, next, &sock->is_ctl_active_list, iscm_link) {
 		list_del(&pos->iscm_link);
+		/* active transfer should be aborted before */
 		list_add(&pos->iscm_link, &sock->is_ctl_idle_list);
 	}
 
 	list_for_each_entry_safe(pos, next, &sock->is_ctl_idle_list, iscm_link) {
 		list_del(&pos->iscm_link);
 
+		ctl_msg_fini(sock, pos);
 		kfree(pos);
 	}
 }
diff --git a/ib-sock-int.h b/ib-sock-int.h
index f1818d8..4eda29d 100644
--- a/ib-sock-int.h
+++ b/ib-sock-int.h
@@ -16,8 +16,11 @@
 #define IB_MAX_PARALLEL	 1
 
 /* 1 RX + 1 TX in flight */
-#define IB_MAX_CTL_MSG	(IB_MAX_PARALLEL * 2)
-#define IB_CQ_EVENTS_MAX (IB_MAX_PARALLEL * 2)
+#define IB_MAX_CTL_MSG		(IB_MAX_PARALLEL * 2)
+#define IB_CQ_EVENTS_MAX	(IB_MAX_PARALLEL * 2)
+/* abstract number, just related to CPU time spent in one event process loop
+ */
+#define IB_CQ_EVENTS_BATCH	(5)
 
 enum ib_sock_flags {
 	SOCK_CONNECTED	= 1 << 0,
@@ -47,7 +50,15 @@ struct IB_SOCK {
 
 	struct ib_sock_mem	is_mem;
 
-	/* transfer related parts */
+	/******* transfer related parts ***************/
+	/* number CQ events in batch poll */
+	struct ib_wc		is_cq_wc[IB_CQ_EVENTS_BATCH];
+	/* we can't process an cq events in callback as 
+	 * it may executed in interrupt context, so create 
+	 * work queue for it. Latter it should be per 
+	 * IB device data */
+	struct work_struct	is_cq_work;
+	
 	/* completion events */
 	struct ib_cq		*is_cq;
 	/* queue pair to communicate between nodes */
@@ -56,11 +67,13 @@ struct IB_SOCK {
 	struct ib_qp		*is_qp;
 
 	/* control messages */
-	/* IDLE <> active protection  */
+	/* IDLE <> active <> rd protection  */
 	spinlock_t		is_ctl_lock;
 	struct list_head	is_ctl_idle_list;
 	struct list_head	is_ctl_active_list;
+	struct list_head	is_ctl_rd_list;
 	wait_queue_head_t	is_ctl_waitq;
+	/******* transfer related parts end ************/
 
 	/* pre-accepted sockets */
 	spinlock_t		is_child_lock;
@@ -84,19 +97,22 @@ void sock_event_set(struct IB_SOCK *sock, unsigned int event)
 #define IB_HELLO_MAGIC 0x9012
 
 struct ib_hello {
-	__u32	magic;
+	uint32_t	magic;
 } WIRE_ATTR;
 
 #define IB_CTL_MSG_MAGIC	0x87154
 
 struct ib_sock_wire_msg {
 	uint32_t	sww_magic;
+	uint32_t	sww_size;
 } WIRE_ATTR;
 
 /**************************** messages on wire ********************/
 
 /************* ib sock control protocol ***************************/
 
+#define CTL_MSG_RX	0x1
+
 struct ib_sock_ctl {
 	struct list_head	iscm_link;
 
@@ -105,16 +121,21 @@ struct ib_sock_ctl {
 	 * later */
 	struct ib_sge		iscm_sge;
 
+	unsigned long		iscm_flags;
+
 	/* used to describe an incomming rdma transfer,
 	 * must be first WR in sending chain */
 	struct ib_sock_wire_msg	iscm_msg;
 };
 /* ctl-msg.c */
 /* init queue and post sort of rx buffer to wait incomming data */
-int ib_sock_ctl_msg_init(struct IB_SOCK *sock);
-void ib_sock_ctl_msg_fini(struct IB_SOCK *sock);
+int ib_sock_ctl_init(struct IB_SOCK *sock);
+void ib_sock_ctl_fini(struct IB_SOCK *sock);
 /* take control message to send an outgoning buffer */
-struct ib_sock_ctl *ib_sock_ctl_idle_take(struct IB_SOCK *sock);
+struct ib_sock_ctl *ib_sock_ctl_take(struct IB_SOCK *sock);
+void ib_sock_ctl_put(struct IB_SOCK *sock, struct ib_sock_ctl *msg);
+int ib_sock_ctl_post(struct IB_SOCK *sock, struct ib_sock_ctl *msg);
+
 
 /* mem.c */
 /* init function responsible to fill an number WR / SGE per socket*/
diff --git a/ib-sock.c b/ib-sock.c
index c29b400..b228605 100644
--- a/ib-sock.c
+++ b/ib-sock.c
@@ -13,6 +13,83 @@ static void ib_cq_event_callback(struct ib_event *cause, void *context)
 	printk("got cq event %d \n", cause->event);
 }
 
+static void ib_sock_handle_rx(struct IB_SOCK *sock, struct ib_wc *wc)
+{
+	struct ib_sock_ctl *msg = (struct ib_sock_ctl *)(uintptr_t)wc->wr_id;
+	struct ib_device *device = sock->is_id->device;
+	int ret;
+
+	/* CPU is owner*/
+	ib_dma_sync_single_for_cpu(device, 
+				   msg->iscm_sge.addr, msg->iscm_sge.length,
+				   DMA_FROM_DEVICE);
+
+	if (wc->status != IB_WC_SUCCESS) {
+		printk("rx status error %d\n", wc->status);
+		goto repost;
+	}
+
+
+	if (msg->iscm_msg.sww_magic != IB_CTL_MSG_MAGIC) {
+		printk("recv magic bad %x\n", msg->iscm_msg.sww_magic);
+		goto repost;
+	}
+
+	/* do processing there */
+	printk("recv maic ok!\n");
+	spin_lock(&sock->is_ctl_lock);
+	list_move(&msg->iscm_link, &sock->is_ctl_rd_list);
+	spin_lock(&sock->is_ctl_lock);
+
+	/* ready for userland */
+	sock_event_set(sock, POLLIN);
+repost:
+	/* repost to processing */
+	ret  = ib_sock_ctl_post(sock, msg);
+	if (ret != 0) 
+		/* probably we need a chanse to report it later ? */
+		printk("Error with summit to rx queue\n");
+}
+
+static void ib_sock_handle_tx(struct IB_SOCK *sock, struct ib_wc *wc)
+{
+	int event = POLLOUT;
+	/* TX event hit when TX done or error hit */
+
+	if (wc->status != IB_WC_SUCCESS)
+		event |= POLLERR;
+	sock_event_set(sock, event);
+}
+
+/* based on ip over ib code  */
+static void ib_sock_cq_work(struct work_struct *work)
+{
+	struct IB_SOCK *sock;
+	int n, i;
+	struct ib_sock_ctl *msg;
+
+	sock = container_of(work, struct IB_SOCK, is_cq_work);
+
+poll_more:
+	n = ib_poll_cq(sock->is_cq, IB_CQ_EVENTS_BATCH, sock->is_cq_wc);
+	for (i = 0; i < n; i++) {
+		msg = (struct ib_sock_ctl *)(uintptr_t)sock->is_cq_wc[i].wr_id;
+		if (msg->iscm_flags & CTL_MSG_RX)
+			ib_sock_handle_rx(sock, &sock->is_cq_wc[i]);
+		else
+			ib_sock_handle_tx(sock, &sock->is_cq_wc[i]);
+
+	}
+
+	/* abstract limit */
+	if (n < (IB_CQ_EVENTS_BATCH / 2)) {
+		if (unlikely(ib_req_notify_cq(sock->is_cq,
+					      IB_CQ_NEXT_COMP |
+					      IB_CQ_REPORT_MISSED_EVENTS)))
+			goto poll_more;
+	}
+
+}
 
 /* have some change states  */
 /* DID we really needs it ? */
@@ -21,6 +98,7 @@ static void ib_cq_callback(struct ib_cq *cq, void *cq_context)
 	struct IB_SOCK *sock = cq_context;
 
 	printk("cq event %p\n", sock);
+	schedule_work(&sock->is_cq_work);
 }
 
 /* creation of CQ/QP isn't needs to create on route event,
@@ -31,7 +109,10 @@ static int ib_sock_cq_qp_create(struct IB_SOCK *sock)
 	struct ib_qp_init_attr	init_attr;
 	int    ret;
 
-	/* event queue */
+	/* XXX need special refactoring to extract per device data */
+	INIT_WORK(&sock->is_cq_work, ib_sock_cq_work);
+
+	/* event queue, may per CPU and per device, not per socket */
 	sock->is_cq = ib_create_cq(cmid->device,
 				   ib_cq_callback,
 				   ib_cq_event_callback,
@@ -80,6 +161,8 @@ static void ib_sock_cq_qp_destroy(struct IB_SOCK *sock)
 {
 	struct rdma_cm_id *cmid = sock->is_id;
 
+	flush_scheduled_work();
+
 	/* XXX is it needs ? */
 	if (cmid != NULL && cmid->qp != NULL)
 		rdma_destroy_qp(cmid);
@@ -104,6 +187,10 @@ static int ib_sock_resource_alloc(struct IB_SOCK *sock)
 	if (ret < 0)
 		return ret;
 
+	ret = ib_sock_ctl_init(sock);
+	if (ret < 0)
+		return ret;
+
 	return 0;
 }
 
@@ -111,6 +198,7 @@ static void ib_sock_resource_free(struct IB_SOCK *sock)
 {
 	ib_sock_cq_qp_destroy(sock);
 	ib_sock_mem_fini(sock);
+	ib_sock_ctl_fini(sock);
 }
 
 static int
@@ -496,3 +584,67 @@ struct IB_SOCK *ib_socket_accept(struct IB_SOCK *parent)
 	printk("Accept returned %p\n", sock);
 	return sock;
 }
+/*****************************************************************************************/
+/* return a size of transfer */
+size_t ib_socket_read_size(struct IB_SOCK *sock)
+{
+	struct ib_sock_ctl *msg;
+	int ret = -EAGAIN;
+
+	/* as we have a single parallel transfer - 
+	 * just enough to check a first element in list */
+	spin_lock(&sock->is_ctl_lock);
+	msg = list_first_entry_or_null(&sock->is_ctl_rd_list,
+				       struct ib_sock_ctl, iscm_link);
+	if (msg != NULL)
+		ret = msg->iscm_msg.sww_size;
+	spin_unlock(&sock->is_ctl_lock);
+
+	return ret;
+}
+
+/* process contol msg and repost */
+int ib_socket_read(struct IB_SOCK *sock, void *buf, size_t size)
+{
+	struct ib_sock_ctl *msg;
+
+	/* as we have a single parallel transfer - 
+	 * just enough to check a first element in list */
+	spin_lock(&sock->is_ctl_lock);
+	msg = list_first_entry_or_null(&sock->is_ctl_rd_list,
+				       struct ib_sock_ctl, iscm_link);
+	if (msg != NULL)
+		list_del(&msg->iscm_link);
+	spin_unlock(&sock->is_ctl_lock);
+	if (msg == NULL)
+		return -EAGAIN;
+
+	return ib_sock_ctl_post(sock, msg);
+}
+
+/* send some amount data over wire */
+int ib_socket_write(struct IB_SOCK *sock, void *buf, size_t size)
+{
+	/* stub. send an CTL msg only */
+	struct ib_sock_ctl *msg;
+	struct ib_send_wr wr = {
+		.opcode = IB_WR_SEND,
+	};
+	struct ib_send_wr *bad;
+	int ret = -EINVAL;
+
+	msg = ib_sock_ctl_take(sock);
+
+	wr.next = NULL;
+	wr.wr_id = (uintptr_t)msg;
+	wr.sg_list = &msg->iscm_sge;
+	wr.num_sge = 1;
+
+	ret = ib_post_send(sock->is_qp, &wr, &bad);
+	/* if OK - will returned in idle list in the callback */
+	if (ret != 0)
+		ib_sock_ctl_put(sock, msg);
+
+	return ret;
+}
+
diff --git a/ib-sock.h b/ib-sock.h
index c74c3df..f6f6c51 100644
--- a/ib-sock.h
+++ b/ib-sock.h
@@ -28,7 +28,18 @@ unsigned long ib_socket_poll(struct IB_SOCK *sock);
 int ib_socket_bind(struct IB_SOCK *sock, uint32_t addr, unsigned port);
 struct IB_SOCK *ib_socket_accept(struct IB_SOCK *parent);
 
+/* return a size of transfer */
+size_t ib_socket_read_size(struct IB_SOCK *sock);
+/* submit to read.
+ * read protocol
+ * first POLLIN event have result to call ib_socket_read_try to get a buffer size
+ *  and call of ib_socket_read will send request to read and attach a buffers
+ * but return a EAGAIN to wait a transfer done
+ * next POLLIN event will inducate a transfer done  */
+int ib_socket_read(struct IB_SOCK *sock, void *buf, size_t size);
+
+
 /* send some amount data over wire */
-size_t ib_socket_write(struct IB_SOCK *sock, void *buf, size_t size);
+int ib_socket_write(struct IB_SOCK *sock, void *buf, size_t size);
 
 #endif
\ No newline at end of file
diff --git a/srv.c b/srv.c
index 6f350e9..ecb8ad8 100644
--- a/srv.c
+++ b/srv.c
@@ -7,6 +7,8 @@ srv_init(void)
 {
 	struct IB_SOCK *sock, *sock_child = NULL;
 	unsigned long event = 0;
+	bool read_st = false;
+	void *data;
 	int ret;
 
 	sock = ib_socket_create();
@@ -28,13 +30,28 @@ srv_init(void)
 	/* have incomming event, so socket is ready */
 	sock_child = ib_socket_accept(sock);
 	BUG_ON(sock_child == NULL);
-#if 0
+
 	while ( 1 ) {
 		event = ib_socket_poll(sock_child);
 		if (event & POLLERR)
 			break;
+		if (!read_st && (event & POLLIN)) {
+			size_t d_size;
+
+			d_size = ib_socket_read_size(sock);
+			data = kmalloc(d_size, GFP_KERNEL);
+			if (data == NULL)
+				break;
+			read_st = true;
+		}
+		if (read_st && (event & POLLIN)) {
+			/* read done */
+			printk("%s\n", data);
+			kfree(data);
+			read_st = false;
+		}
 	}
-#endif
+
 	ib_socket_destroy(sock_child);
 out:
 	ib_socket_destroy(sock);
